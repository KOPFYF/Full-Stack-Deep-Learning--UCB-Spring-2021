{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4_n_Lab5-Transformer_Experiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8H-F4rm8OSa",
        "outputId": "51b77b69-ea95-46f8-d02d-d0ee3656b4b1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Aug 21 23:00:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpOST4fR8gQj",
        "outputId": "bf6f4bf1-1372-4b3f-b2a2-0a745408552e"
      },
      "source": [
        "# FSDL Spring 2021 Setup\n",
        "!git clone https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs\n",
        "%cd fsdl-text-recognizer-2021-labs\n",
        "!pip3 install boltons wandb pytorch_lightning==1.1.4 pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 torchtext==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "%env PYTHONPATH=.:$PYTHONPATH"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fsdl-text-recognizer-2021-labs'...\n",
            "remote: Enumerating objects: 798, done.\u001b[K\n",
            "remote: Counting objects: 100% (232/232), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 798 (delta 158), reused 144 (delta 140), pack-reused 566\u001b[K\n",
            "Receiving objects: 100% (798/798), 18.89 MiB | 29.22 MiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n",
            "/content/fsdl-text-recognizer-2021-labs\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting boltons\n",
            "  Downloading boltons-21.0.0-py2.py3-none-any.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 67.3 MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning==1.1.4\n",
            "  Downloading pytorch_lightning-1.1.4-py3-none-any.whl (684 kB)\n",
            "\u001b[K     |████████████████████████████████| 684 kB 64.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting install\n",
            "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
            "Collecting torch==1.7.1+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8 MB)\n",
            "\u001b[K     |███████████████████████         | 834.1 MB 1.3 MB/s eta 0:04:16tcmalloc: large alloc 1147494400 bytes == 0x55ac577c4000 @  0x7f15b3a8a615 0x55ac1ed1102c 0x55ac1edf117a 0x55ac1ed13e4d 0x55ac1ee05c0d 0x55ac1ed880d8 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed87f40 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed8493b 0x55ac1ee06a56 0x55ac1ed83fb3 0x55ac1ee06a56 0x55ac1ed83fb3 0x55ac1ee06a56 0x55ac1ed83fb3 0x55ac1ed15b99 0x55ac1ed58e79 0x55ac1ed147b2 0x55ac1ed87e65 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed8493b 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed83b0e 0x55ac1ed1565a 0x55ac1ed83d67 0x55ac1ed82c35\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7 MB 1.2 MB/s eta 0:01:25tcmalloc: large alloc 1434370048 bytes == 0x55ac9be1a000 @  0x7f15b3a8a615 0x55ac1ed1102c 0x55ac1edf117a 0x55ac1ed13e4d 0x55ac1ee05c0d 0x55ac1ed880d8 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed87f40 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed8493b 0x55ac1ee06a56 0x55ac1ed83fb3 0x55ac1ee06a56 0x55ac1ed83fb3 0x55ac1ee06a56 0x55ac1ed83fb3 0x55ac1ed15b99 0x55ac1ed58e79 0x55ac1ed147b2 0x55ac1ed87e65 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed8493b 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed83b0e 0x55ac1ed1565a 0x55ac1ed83d67 0x55ac1ed82c35\n",
            "\u001b[K     |████████████████████████████████| 1156.7 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x55acf1606000 @  0x7f15b3a8a615 0x55ac1ed1102c 0x55ac1edf117a 0x55ac1ed13e4d 0x55ac1ee05c0d 0x55ac1ed880d8 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed83d67 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed83d67 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed83d67 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed83d67 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed83d67 0x55ac1ed1565a 0x55ac1ed83d67 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed8493b 0x55ac1ed82c35 0x55ac1ed1573a 0x55ac1ed8493b 0x55ac1ed82c35 0x55ac1ed15dd1\n",
            "\u001b[K     |████████████████████████████████| 1156.8 MB 14 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 173 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "  Downloading torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 71.8 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.8.1\n",
            "  Downloading torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 22.9 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 77.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (4.62.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.1.4) (2.6.0)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.1) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.39.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.37.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.4.5)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.34.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.1.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 79.0 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning==1.1.4) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 63.1 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning==1.1.4) (3.5.0)\n",
            "Building wheels for collected packages: future, subprocess32, pathtools\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=6ca671574de2c9f8832dfc7b1d49cbcc699b392f97e3c8c6e5605d5361ce5a46\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=7c7cbb15bf4da1fd39d12d3ee7e3441744ac6da0c2efe8c5d077c3271045319c\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=bc7db2fe0859ec0f967422f4cc3a27f46e557c201be2ff8c5ff8484bb84bffeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built future subprocess32 pathtools\n",
            "Installing collected packages: multidict, yarl, smmap, async-timeout, gitdb, fsspec, aiohttp, torch, subprocess32, shortuuid, sentry-sdk, PyYAML, pathtools, GitPython, future, docker-pycreds, configparser, wandb, torchvision, torchtext, torchaudio, pytorch-lightning, install, boltons\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "Successfully installed GitPython-3.1.18 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 boltons-21.0.0 configparser-5.0.2 docker-pycreds-0.4.0 fsspec-2021.7.0 future-0.18.2 gitdb-4.0.7 install-1.3.4 multidict-5.1.0 pathtools-0.1.2 pytorch-lightning-1.1.4 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 torch-1.7.1+cu110 torchaudio-0.7.2 torchtext-0.8.1 torchvision-0.8.2+cu110 wandb-0.12.0 yarl-1.6.3\n",
            "env: PYTHONPATH=.:$PYTHONPATH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ujUZtwL8ifP",
        "outputId": "e91be51b-f13f-49b8-a504-02e4390c8c5a"
      },
      "source": [
        "%cd lab4/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fsdl-text-recognizer-2021-labs/lab4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iPb_2c58mKZ",
        "outputId": "ec3d2aa3-c00d-427d-e9a6-cd30b001ed0c"
      },
      "source": [
        "# more epochs are necessary with the Transformer than with our LSTM+CTC model. \n",
        "# ~30 epochs gives the same performance as we were able to obtain before\n",
        "!python training/run_experiment.py --max_epochs=40 --gpus=1 --num_workers=16 --data_class=EMNISTLines --min_overlap=0 --max_overlap=0.33 \\\n",
        "                                   --model_class=LineCNNTransformer --window_width=20 --window_stride=12 --loss=transformer\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You have set progress_bar_refresh_rate < 20 on Google Colab. This may crash. Consider using progress_bar_refresh_rate >= 20 in Trainer.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "EMNISTLinesDataset generating data for train...\n",
            "[nltk_data] Downloading package brown to /content/fsdl-text-\n",
            "[nltk_data]     recognizer-2021-labs/data/downloaded/nltk...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "Downloading raw dataset from https://s3-us-west-2.amazonaws.com/fsdl-public-assets/matlab.zip to /content/fsdl-text-recognizer-2021-labs/data/downloaded/emnist/matlab.zip...\n",
            "709MB [00:09, 77.8MB/s]               \n",
            "Computing SHA-256...\n",
            "Unzipping EMNIST...\n",
            "Loading training data from .mat file\n",
            "Balancing classes to reduce amount of data\n",
            "Saving to HDF5 in a compressed format...\n",
            "Saving essential dataset parameters to text_recognizer/datasets...\n",
            "Cleaning up...\n",
            "EMNISTLinesDataset generating data for val...\n",
            "EMNISTLinesDataset generating data for test...\n",
            "EMNISTLinesDataset loading data from HDF5...\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 3.8 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 1.1 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 698 K \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 393 K \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 393 K \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 131 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 2.6 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 2.6 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 659 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 65.8 K\n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 65.8 K\n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 659 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 65.8 K\n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 65.8 K\n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 659 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 65.8 K\n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 65.8 K\n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 659 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 65.8 K\n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 65.8 K\n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "3.8 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.8 M     Total params\n",
            "Epoch 0:  83% 79/95 [00:27<00:05,  2.84it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  85% 81/95 [00:29<00:05,  2.75it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Validating:  12% 2/16 [00:02<00:14,  1.02s/it]\u001b[A\n",
            "Epoch 0:  87% 83/95 [00:30<00:04,  2.71it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Validating:  25% 4/16 [00:03<00:08,  1.36it/s]\u001b[A\n",
            "Epoch 0:  89% 85/95 [00:31<00:03,  2.67it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Validating:  38% 6/16 [00:04<00:06,  1.53it/s]\u001b[A\n",
            "Epoch 0:  92% 87/95 [00:32<00:03,  2.64it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Validating:  50% 8/16 [00:05<00:05,  1.59it/s]\u001b[A\n",
            "Epoch 0:  94% 89/95 [00:34<00:02,  2.60it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Validating:  62% 10/16 [00:06<00:03,  1.64it/s]\u001b[A\n",
            "Epoch 0:  96% 91/95 [00:35<00:01,  2.57it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Validating:  75% 12/16 [00:08<00:02,  1.66it/s]\u001b[A\n",
            "Epoch 0:  98% 93/95 [00:36<00:00,  2.54it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Validating:  88% 14/16 [00:09<00:01,  1.66it/s]\u001b[A\n",
            "Epoch 0: 100% 95/95 [00:37<00:00,  2.52it/s, loss=3.01, v_num=0, val_loss=4.99, val_cer=0.993]\n",
            "Epoch 0: 100% 95/95 [00:39<00:00,  2.44it/s, loss=3.01, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Epoch 1:  84% 80/95 [00:28<00:05,  2.84it/s, loss=2.43, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 1:  86% 82/95 [00:30<00:04,  2.68it/s, loss=2.43, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.17it/s]\u001b[A\n",
            "Epoch 1:  88% 84/95 [00:31<00:04,  2.64it/s, loss=2.43, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.43it/s]\u001b[A\n",
            "Epoch 1:  91% 86/95 [00:32<00:03,  2.61it/s, loss=2.43, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Validating:  44% 7/16 [00:05<00:05,  1.55it/s]\u001b[A\n",
            "Epoch 1:  93% 88/95 [00:34<00:02,  2.57it/s, loss=2.43, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.60it/s]\u001b[A\n",
            "Epoch 1:  95% 90/95 [00:35<00:01,  2.54it/s, loss=2.43, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Validating:  69% 11/16 [00:07<00:03,  1.63it/s]\u001b[A\n",
            "Epoch 1:  97% 92/95 [00:36<00:01,  2.51it/s, loss=2.43, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Validating:  81% 13/16 [00:08<00:01,  1.64it/s]\u001b[A\n",
            "Epoch 1:  99% 94/95 [00:37<00:00,  2.49it/s, loss=2.43, v_num=0, val_loss=2.84, val_cer=0.936]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.65it/s]\u001b[A\n",
            "Epoch 1: 100% 95/95 [00:39<00:00,  2.40it/s, loss=2.43, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Epoch 2:  84% 80/95 [00:28<00:05,  2.83it/s, loss=2.32, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.79s/it]\u001b[A\n",
            "Epoch 2:  86% 82/95 [00:30<00:04,  2.67it/s, loss=2.32, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 2:  88% 84/95 [00:31<00:04,  2.63it/s, loss=2.32, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.40it/s]\u001b[A\n",
            "Epoch 2:  91% 86/95 [00:33<00:03,  2.59it/s, loss=2.32, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Validating:  44% 7/16 [00:05<00:05,  1.53it/s]\u001b[A\n",
            "Epoch 2:  93% 88/95 [00:34<00:02,  2.56it/s, loss=2.32, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.59it/s]\u001b[A\n",
            "Epoch 2:  95% 90/95 [00:35<00:01,  2.53it/s, loss=2.32, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Validating:  69% 11/16 [00:07<00:03,  1.62it/s]\u001b[A\n",
            "Epoch 2:  97% 92/95 [00:36<00:01,  2.50it/s, loss=2.32, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.63it/s]\u001b[A\n",
            "Epoch 2:  99% 94/95 [00:38<00:00,  2.47it/s, loss=2.32, v_num=0, val_loss=2.32, val_cer=0.835]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.63it/s]\u001b[A\n",
            "Epoch 2: 100% 95/95 [00:39<00:00,  2.38it/s, loss=2.32, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Epoch 3:  84% 80/95 [00:28<00:05,  2.79it/s, loss=2.24, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 3:  86% 82/95 [00:30<00:04,  2.65it/s, loss=2.24, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.16it/s]\u001b[A\n",
            "Epoch 3:  88% 84/95 [00:32<00:04,  2.61it/s, loss=2.24, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.41it/s]\u001b[A\n",
            "Epoch 3:  91% 86/95 [00:33<00:03,  2.57it/s, loss=2.24, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Validating:  44% 7/16 [00:05<00:05,  1.52it/s]\u001b[A\n",
            "Epoch 3:  93% 88/95 [00:34<00:02,  2.54it/s, loss=2.24, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.58it/s]\u001b[A\n",
            "Epoch 3:  95% 90/95 [00:35<00:01,  2.51it/s, loss=2.24, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Validating:  69% 11/16 [00:07<00:03,  1.60it/s]\u001b[A\n",
            "Epoch 3:  97% 92/95 [00:37<00:01,  2.48it/s, loss=2.24, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.61it/s]\u001b[A\n",
            "Epoch 3:  99% 94/95 [00:38<00:00,  2.45it/s, loss=2.24, v_num=0, val_loss=2.19, val_cer=0.835]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.62it/s]\u001b[A\n",
            "Epoch 3: 100% 95/95 [00:40<00:00,  2.36it/s, loss=2.24, v_num=0, val_loss=2.1, val_cer=0.825] \n",
            "Epoch 4:  84% 80/95 [00:28<00:05,  2.77it/s, loss=2.16, v_num=0, val_loss=2.1, val_cer=0.825]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 4:  86% 82/95 [00:31<00:04,  2.62it/s, loss=2.16, v_num=0, val_loss=2.1, val_cer=0.825]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.15it/s]\u001b[A\n",
            "Epoch 4:  88% 84/95 [00:32<00:04,  2.59it/s, loss=2.16, v_num=0, val_loss=2.1, val_cer=0.825]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.39it/s]\u001b[A\n",
            "Epoch 4:  91% 86/95 [00:33<00:03,  2.55it/s, loss=2.16, v_num=0, val_loss=2.1, val_cer=0.825]\n",
            "Validating:  44% 7/16 [00:05<00:05,  1.51it/s]\u001b[A\n",
            "Epoch 4:  93% 88/95 [00:34<00:02,  2.52it/s, loss=2.16, v_num=0, val_loss=2.1, val_cer=0.825]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.57it/s]\u001b[A\n",
            "Epoch 4:  95% 90/95 [00:36<00:02,  2.49it/s, loss=2.16, v_num=0, val_loss=2.1, val_cer=0.825]\n",
            "Validating:  69% 11/16 [00:07<00:03,  1.60it/s]\u001b[A\n",
            "Epoch 4:  97% 92/95 [00:37<00:01,  2.46it/s, loss=2.16, v_num=0, val_loss=2.1, val_cer=0.825]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.61it/s]\u001b[A\n",
            "Epoch 4:  99% 94/95 [00:38<00:00,  2.43it/s, loss=2.16, v_num=0, val_loss=2.1, val_cer=0.825]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.61it/s]\u001b[A\n",
            "Epoch 4: 100% 95/95 [00:40<00:00,  2.35it/s, loss=2.16, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Epoch 5:  84% 80/95 [00:29<00:05,  2.75it/s, loss=2.08, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.77s/it]\u001b[A\n",
            "Epoch 5:  86% 82/95 [00:31<00:04,  2.61it/s, loss=2.08, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 5:  88% 84/95 [00:32<00:04,  2.57it/s, loss=2.08, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.39it/s]\u001b[A\n",
            "Epoch 5:  91% 86/95 [00:33<00:03,  2.54it/s, loss=2.08, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.50it/s]\u001b[A\n",
            "Epoch 5:  93% 88/95 [00:35<00:02,  2.50it/s, loss=2.08, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.56it/s]\u001b[A\n",
            "Epoch 5:  95% 90/95 [00:36<00:02,  2.47it/s, loss=2.08, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Validating:  69% 11/16 [00:07<00:03,  1.58it/s]\u001b[A\n",
            "Epoch 5:  97% 92/95 [00:37<00:01,  2.44it/s, loss=2.08, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.59it/s]\u001b[A\n",
            "Epoch 5:  99% 94/95 [00:38<00:00,  2.42it/s, loss=2.08, v_num=0, val_loss=2.04, val_cer=0.834]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.60it/s]\u001b[A\n",
            "Epoch 5: 100% 95/95 [00:40<00:00,  2.33it/s, loss=2.08, v_num=0, val_loss=1.95, val_cer=0.83] \n",
            "Epoch 6:  84% 80/95 [00:29<00:05,  2.75it/s, loss=1.97, v_num=0, val_loss=1.95, val_cer=0.83]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.77s/it]\u001b[A\n",
            "Epoch 6:  86% 82/95 [00:31<00:04,  2.60it/s, loss=1.97, v_num=0, val_loss=1.95, val_cer=0.83]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 6:  88% 84/95 [00:32<00:04,  2.56it/s, loss=1.97, v_num=0, val_loss=1.95, val_cer=0.83]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.39it/s]\u001b[A\n",
            "Epoch 6:  91% 86/95 [00:34<00:03,  2.53it/s, loss=1.97, v_num=0, val_loss=1.95, val_cer=0.83]\n",
            "Validating:  44% 7/16 [00:05<00:05,  1.50it/s]\u001b[A\n",
            "Epoch 6:  93% 88/95 [00:35<00:02,  2.49it/s, loss=1.97, v_num=0, val_loss=1.95, val_cer=0.83]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 6:  95% 90/95 [00:36<00:02,  2.46it/s, loss=1.97, v_num=0, val_loss=1.95, val_cer=0.83]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.58it/s]\u001b[A\n",
            "Epoch 6:  97% 92/95 [00:37<00:01,  2.44it/s, loss=1.97, v_num=0, val_loss=1.95, val_cer=0.83]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.59it/s]\u001b[A\n",
            "Epoch 6:  99% 94/95 [00:39<00:00,  2.41it/s, loss=1.97, v_num=0, val_loss=1.95, val_cer=0.83]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.60it/s]\u001b[A\n",
            "Epoch 6: 100% 95/95 [00:40<00:00,  2.33it/s, loss=1.97, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Epoch 7:  84% 80/95 [00:29<00:05,  2.75it/s, loss=1.81, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.78s/it]\u001b[A\n",
            "Epoch 7:  86% 82/95 [00:31<00:04,  2.60it/s, loss=1.81, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 7:  88% 84/95 [00:32<00:04,  2.57it/s, loss=1.81, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 7:  91% 86/95 [00:33<00:03,  2.53it/s, loss=1.81, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.50it/s]\u001b[A\n",
            "Epoch 7:  93% 88/95 [00:35<00:02,  2.50it/s, loss=1.81, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 7:  95% 90/95 [00:36<00:02,  2.47it/s, loss=1.81, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.58it/s]\u001b[A\n",
            "Epoch 7:  97% 92/95 [00:37<00:01,  2.44it/s, loss=1.81, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.59it/s]\u001b[A\n",
            "Epoch 7:  99% 94/95 [00:38<00:00,  2.41it/s, loss=1.81, v_num=0, val_loss=1.84, val_cer=0.828]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.60it/s]\u001b[A\n",
            "Epoch 7: 100% 95/95 [00:40<00:00,  2.33it/s, loss=1.81, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Epoch 8:  84% 80/95 [00:29<00:05,  2.74it/s, loss=1.29, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 8:  86% 82/95 [00:31<00:05,  2.60it/s, loss=1.29, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 8:  88% 84/95 [00:32<00:04,  2.56it/s, loss=1.29, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.39it/s]\u001b[A\n",
            "Epoch 8:  91% 86/95 [00:34<00:03,  2.52it/s, loss=1.29, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.50it/s]\u001b[A\n",
            "Epoch 8:  93% 88/95 [00:35<00:02,  2.49it/s, loss=1.29, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 8:  95% 90/95 [00:36<00:02,  2.46it/s, loss=1.29, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Validating:  69% 11/16 [00:07<00:03,  1.58it/s]\u001b[A\n",
            "Epoch 8:  97% 92/95 [00:37<00:01,  2.43it/s, loss=1.29, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.59it/s]\u001b[A\n",
            "Epoch 8:  99% 94/95 [00:39<00:00,  2.41it/s, loss=1.29, v_num=0, val_loss=1.58, val_cer=0.796]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 8: 100% 95/95 [00:40<00:00,  2.32it/s, loss=1.29, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Epoch 9:  84% 80/95 [00:29<00:05,  2.72it/s, loss=0.841, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.77s/it]\u001b[A\n",
            "Epoch 9:  86% 82/95 [00:31<00:05,  2.58it/s, loss=0.841, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 9:  88% 84/95 [00:33<00:04,  2.54it/s, loss=0.841, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 9:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.841, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.50it/s]\u001b[A\n",
            "Epoch 9:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.841, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 9:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.841, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.58it/s]\u001b[A\n",
            "Epoch 9:  97% 92/95 [00:38<00:01,  2.42it/s, loss=0.841, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 9:  99% 94/95 [00:39<00:00,  2.39it/s, loss=0.841, v_num=0, val_loss=0.918, val_cer=0.725]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 9: 100% 95/95 [00:41<00:00,  2.31it/s, loss=0.841, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Epoch 10:  84% 80/95 [00:29<00:05,  2.74it/s, loss=0.568, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.77s/it]\u001b[A\n",
            "Epoch 10:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.568, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 10:  88% 84/95 [00:32<00:04,  2.56it/s, loss=0.568, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 10:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.568, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 10:  93% 88/95 [00:35<00:02,  2.49it/s, loss=0.568, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 10:  95% 90/95 [00:36<00:02,  2.46it/s, loss=0.568, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 10:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.568, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 10:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.568, v_num=0, val_loss=0.584, val_cer=0.664]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 10: 100% 95/95 [00:40<00:00,  2.32it/s, loss=0.568, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Epoch 11:  84% 80/95 [00:29<00:05,  2.74it/s, loss=0.435, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.76s/it]\u001b[A\n",
            "Epoch 11:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.435, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 11:  88% 84/95 [00:32<00:04,  2.56it/s, loss=0.435, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 11:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.435, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 11:  93% 88/95 [00:35<00:02,  2.49it/s, loss=0.435, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 11:  95% 90/95 [00:36<00:02,  2.46it/s, loss=0.435, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 11:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.435, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.59it/s]\u001b[A\n",
            "Epoch 11:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.435, v_num=0, val_loss=0.375, val_cer=0.627]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 11: 100% 95/95 [00:40<00:00,  2.32it/s, loss=0.435, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Epoch 12:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.354, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 12:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.354, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 12:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.354, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 12:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.354, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 12:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.354, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 12:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.354, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.58it/s]\u001b[A\n",
            "Epoch 12:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.354, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.59it/s]\u001b[A\n",
            "Epoch 12:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.354, v_num=0, val_loss=0.361, val_cer=0.622]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.60it/s]\u001b[A\n",
            "Epoch 12: 100% 95/95 [00:41<00:00,  2.32it/s, loss=0.354, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Epoch 13:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.312, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.78s/it]\u001b[A\n",
            "Epoch 13:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.312, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 13:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.312, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 13:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.312, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 13:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.312, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 13:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.312, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 13:  97% 92/95 [00:37<00:01,  2.42it/s, loss=0.312, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 13:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.312, v_num=0, val_loss=0.235, val_cer=0.603]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.58it/s]\u001b[A\n",
            "Epoch 13: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.312, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Epoch 14:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.253, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.77s/it]\u001b[A\n",
            "Epoch 14:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.253, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 14:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.253, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 14:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.253, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 14:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.253, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 14:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.253, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 14:  97% 92/95 [00:37<00:01,  2.42it/s, loss=0.253, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 14:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.253, v_num=0, val_loss=0.278, val_cer=0.606]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 14: 100% 95/95 [00:41<00:00,  2.31it/s, loss=0.253, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Epoch 15:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.247, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:27,  1.80s/it]\u001b[A\n",
            "Epoch 15:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.247, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.12it/s]\u001b[A\n",
            "Epoch 15:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.247, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Validating:  31% 5/16 [00:04<00:08,  1.37it/s]\u001b[A\n",
            "Epoch 15:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.247, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.48it/s]\u001b[A\n",
            "Epoch 15:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.247, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 15:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.247, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.56it/s]\u001b[A\n",
            "Epoch 15:  97% 92/95 [00:37<00:01,  2.42it/s, loss=0.247, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 15:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.247, v_num=0, val_loss=0.197, val_cer=0.592]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 15: 100% 95/95 [00:41<00:00,  2.31it/s, loss=0.247, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Epoch 16:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.237, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.77s/it]\u001b[A\n",
            "Epoch 16:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.237, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 16:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.237, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 16:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.237, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 16:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.237, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 16:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.237, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 16:  97% 92/95 [00:37<00:01,  2.42it/s, loss=0.237, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 16:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.237, v_num=0, val_loss=0.178, val_cer=0.591]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 16: 100% 95/95 [00:40<00:00,  2.32it/s, loss=0.237, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Epoch 17:  84% 80/95 [00:29<00:05,  2.74it/s, loss=0.199, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.75s/it]\u001b[A\n",
            "Epoch 17:  86% 82/95 [00:31<00:05,  2.60it/s, loss=0.199, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 17:  88% 84/95 [00:32<00:04,  2.56it/s, loss=0.199, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 17:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.199, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 17:  93% 88/95 [00:35<00:02,  2.49it/s, loss=0.199, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 17:  95% 90/95 [00:36<00:02,  2.46it/s, loss=0.199, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.56it/s]\u001b[A\n",
            "Epoch 17:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.199, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 17:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.199, v_num=0, val_loss=0.189, val_cer=0.591]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.58it/s]\u001b[A\n",
            "Epoch 17: 100% 95/95 [00:41<00:00,  2.32it/s, loss=0.199, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Epoch 18:  84% 80/95 [00:29<00:05,  2.72it/s, loss=0.19, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.79s/it]\u001b[A\n",
            "Epoch 18:  86% 82/95 [00:31<00:05,  2.58it/s, loss=0.19, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.12it/s]\u001b[A\n",
            "Epoch 18:  88% 84/95 [00:33<00:04,  2.54it/s, loss=0.19, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Validating:  31% 5/16 [00:04<00:08,  1.37it/s]\u001b[A\n",
            "Epoch 18:  91% 86/95 [00:34<00:03,  2.50it/s, loss=0.19, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 18:  93% 88/95 [00:35<00:02,  2.47it/s, loss=0.19, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 18:  95% 90/95 [00:36<00:02,  2.44it/s, loss=0.19, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 18:  97% 92/95 [00:38<00:01,  2.41it/s, loss=0.19, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 18:  99% 94/95 [00:39<00:00,  2.39it/s, loss=0.19, v_num=0, val_loss=0.169, val_cer=0.589]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 18: 100% 95/95 [00:40<00:00,  2.32it/s, loss=0.19, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Epoch 19:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.174, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.76s/it]\u001b[A\n",
            "Epoch 19:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.174, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 19:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.174, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 19:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.174, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 19:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.174, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 19:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.174, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.56it/s]\u001b[A\n",
            "Epoch 19:  97% 92/95 [00:38<00:01,  2.42it/s, loss=0.174, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 19:  99% 94/95 [00:39<00:00,  2.39it/s, loss=0.174, v_num=0, val_loss=0.195, val_cer=0.593]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 19: 100% 95/95 [00:40<00:00,  2.32it/s, loss=0.174, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Epoch 20:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.181, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.77s/it]\u001b[A\n",
            "Epoch 20:  86% 82/95 [00:31<00:05,  2.58it/s, loss=0.181, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 20:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.181, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 20:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.181, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 20:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.181, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 20:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.181, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 20:  97% 92/95 [00:37<00:01,  2.42it/s, loss=0.181, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 20:  99% 94/95 [00:39<00:00,  2.39it/s, loss=0.181, v_num=0, val_loss=0.191, val_cer=0.594]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 20: 100% 95/95 [00:40<00:00,  2.32it/s, loss=0.181, v_num=0, val_loss=0.17, val_cer=0.591] \n",
            "Epoch 21:  84% 80/95 [00:29<00:05,  2.72it/s, loss=0.156, v_num=0, val_loss=0.17, val_cer=0.591]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.79s/it]\u001b[A\n",
            "Epoch 21:  86% 82/95 [00:31<00:05,  2.58it/s, loss=0.156, v_num=0, val_loss=0.17, val_cer=0.591]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.13it/s]\u001b[A\n",
            "Epoch 21:  88% 84/95 [00:33<00:04,  2.54it/s, loss=0.156, v_num=0, val_loss=0.17, val_cer=0.591]\n",
            "Validating:  31% 5/16 [00:04<00:08,  1.37it/s]\u001b[A\n",
            "Epoch 21:  91% 86/95 [00:34<00:03,  2.50it/s, loss=0.156, v_num=0, val_loss=0.17, val_cer=0.591]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.48it/s]\u001b[A\n",
            "Epoch 21:  93% 88/95 [00:35<00:02,  2.47it/s, loss=0.156, v_num=0, val_loss=0.17, val_cer=0.591]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 21:  95% 90/95 [00:36<00:02,  2.44it/s, loss=0.156, v_num=0, val_loss=0.17, val_cer=0.591]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 21:  97% 92/95 [00:38<00:01,  2.41it/s, loss=0.156, v_num=0, val_loss=0.17, val_cer=0.591]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 21:  99% 94/95 [00:39<00:00,  2.39it/s, loss=0.156, v_num=0, val_loss=0.17, val_cer=0.591]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 21: 100% 95/95 [00:41<00:00,  2.30it/s, loss=0.156, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Epoch 22:  84% 80/95 [00:29<00:05,  2.72it/s, loss=0.146, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.76s/it]\u001b[A\n",
            "Epoch 22:  86% 82/95 [00:31<00:05,  2.58it/s, loss=0.146, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 22:  88% 84/95 [00:33<00:04,  2.54it/s, loss=0.146, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 22:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.146, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 22:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.146, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 22:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.146, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 22:  97% 92/95 [00:38<00:01,  2.42it/s, loss=0.146, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 22:  99% 94/95 [00:39<00:00,  2.39it/s, loss=0.146, v_num=0, val_loss=0.16, val_cer=0.587]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 22: 100% 95/95 [00:41<00:00,  2.31it/s, loss=0.146, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Epoch 23:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.15, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 23:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.15, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 23:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.15, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 23:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.15, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 23:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.15, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 23:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.15, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 23:  97% 92/95 [00:37<00:01,  2.42it/s, loss=0.15, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 23:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.15, v_num=0, val_loss=0.159, val_cer=0.586]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 23: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.15, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Epoch 24:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.142, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.76s/it]\u001b[A\n",
            "Epoch 24:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.142, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 24:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.142, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 24:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.142, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 24:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.142, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 24:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.142, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.56it/s]\u001b[A\n",
            "Epoch 24:  97% 92/95 [00:37<00:01,  2.42it/s, loss=0.142, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 24:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.142, v_num=0, val_loss=0.164, val_cer=0.588]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.58it/s]\u001b[A\n",
            "Epoch 24: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.142, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Epoch 25:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.139, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.75s/it]\u001b[A\n",
            "Epoch 25:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.139, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 25:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.139, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 25:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.139, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 25:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.139, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 25:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.139, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 25:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.139, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 25:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.139, v_num=0, val_loss=0.166, val_cer=0.588]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 25: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.139, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Epoch 26:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.129, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 26:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.129, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 26:  88% 84/95 [00:32<00:04,  2.56it/s, loss=0.129, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 26:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.129, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 26:  93% 88/95 [00:35<00:02,  2.49it/s, loss=0.129, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 26:  95% 90/95 [00:36<00:02,  2.46it/s, loss=0.129, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 26:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.129, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 26:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.129, v_num=0, val_loss=0.173, val_cer=0.589]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 26: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.129, v_num=0, val_loss=0.16, val_cer=0.589] \n",
            "Epoch 27:  84% 80/95 [00:29<00:05,  2.74it/s, loss=0.117, v_num=0, val_loss=0.16, val_cer=0.589]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.75s/it]\u001b[A\n",
            "Epoch 27:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.117, v_num=0, val_loss=0.16, val_cer=0.589]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 27:  88% 84/95 [00:32<00:04,  2.56it/s, loss=0.117, v_num=0, val_loss=0.16, val_cer=0.589]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 27:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.117, v_num=0, val_loss=0.16, val_cer=0.589]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.50it/s]\u001b[A\n",
            "Epoch 27:  93% 88/95 [00:35<00:02,  2.49it/s, loss=0.117, v_num=0, val_loss=0.16, val_cer=0.589]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 27:  95% 90/95 [00:36<00:02,  2.46it/s, loss=0.117, v_num=0, val_loss=0.16, val_cer=0.589]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 27:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.117, v_num=0, val_loss=0.16, val_cer=0.589]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.59it/s]\u001b[A\n",
            "Epoch 27:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.117, v_num=0, val_loss=0.16, val_cer=0.589]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 27: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.117, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Epoch 28:  84% 80/95 [00:29<00:05,  2.74it/s, loss=0.13, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.75s/it]\u001b[A\n",
            "Epoch 28:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.13, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 28:  88% 84/95 [00:32<00:04,  2.56it/s, loss=0.13, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 28:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.13, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.50it/s]\u001b[A\n",
            "Epoch 28:  93% 88/95 [00:35<00:02,  2.49it/s, loss=0.13, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 28:  95% 90/95 [00:36<00:02,  2.46it/s, loss=0.13, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 28:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.13, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 28:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.13, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 28: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.13, v_num=0, val_loss=0.16, val_cer=0.585] \n",
            "Epoch 29:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.119, v_num=0, val_loss=0.16, val_cer=0.585]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.76s/it]\u001b[A\n",
            "Epoch 29:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.119, v_num=0, val_loss=0.16, val_cer=0.585]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 29:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.119, v_num=0, val_loss=0.16, val_cer=0.585]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 29:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.119, v_num=0, val_loss=0.16, val_cer=0.585]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 29:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.119, v_num=0, val_loss=0.16, val_cer=0.585]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 29:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.119, v_num=0, val_loss=0.16, val_cer=0.585]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 29:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.119, v_num=0, val_loss=0.16, val_cer=0.585]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 29:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.119, v_num=0, val_loss=0.16, val_cer=0.585]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 29: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.119, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Epoch 30:  84% 80/95 [00:29<00:05,  2.72it/s, loss=0.112, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 30:  86% 82/95 [00:31<00:05,  2.58it/s, loss=0.112, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 30:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.112, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 30:  91% 86/95 [00:34<00:03,  2.51it/s, loss=0.112, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 30:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.112, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 30:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.112, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 30:  97% 92/95 [00:38<00:01,  2.42it/s, loss=0.112, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 30:  99% 94/95 [00:39<00:00,  2.39it/s, loss=0.112, v_num=0, val_loss=0.176, val_cer=0.588]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 30: 100% 95/95 [00:40<00:00,  2.32it/s, loss=0.112, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Epoch 31:  84% 80/95 [00:29<00:05,  2.73it/s, loss=0.111, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.74s/it]\u001b[A\n",
            "Epoch 31:  86% 82/95 [00:31<00:05,  2.59it/s, loss=0.111, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  19% 3/16 [00:02<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 31:  88% 84/95 [00:32<00:04,  2.55it/s, loss=0.111, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 31:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.111, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.50it/s]\u001b[A\n",
            "Epoch 31:  93% 88/95 [00:35<00:02,  2.48it/s, loss=0.111, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.55it/s]\u001b[A\n",
            "Epoch 31:  95% 90/95 [00:36<00:02,  2.45it/s, loss=0.111, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 31:  97% 92/95 [00:37<00:01,  2.42it/s, loss=0.111, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 31:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.111, v_num=0, val_loss=0.163, val_cer=0.585]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 31: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.111, v_num=0, val_loss=0.16, val_cer=0.586] \n",
            "Epoch 32:  84% 80/95 [00:29<00:05,  2.74it/s, loss=0.121, v_num=0, val_loss=0.16, val_cer=0.586]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:01<00:26,  1.75s/it]\u001b[A\n",
            "Epoch 32:  86% 82/95 [00:31<00:05,  2.60it/s, loss=0.121, v_num=0, val_loss=0.16, val_cer=0.586]\n",
            "Validating:  19% 3/16 [00:03<00:11,  1.14it/s]\u001b[A\n",
            "Epoch 32:  88% 84/95 [00:32<00:04,  2.56it/s, loss=0.121, v_num=0, val_loss=0.16, val_cer=0.586]\n",
            "Validating:  31% 5/16 [00:04<00:07,  1.38it/s]\u001b[A\n",
            "Epoch 32:  91% 86/95 [00:34<00:03,  2.52it/s, loss=0.121, v_num=0, val_loss=0.16, val_cer=0.586]\n",
            "Validating:  44% 7/16 [00:05<00:06,  1.49it/s]\u001b[A\n",
            "Epoch 32:  93% 88/95 [00:35<00:02,  2.49it/s, loss=0.121, v_num=0, val_loss=0.16, val_cer=0.586]\n",
            "Validating:  56% 9/16 [00:06<00:04,  1.54it/s]\u001b[A\n",
            "Epoch 32:  95% 90/95 [00:36<00:02,  2.46it/s, loss=0.121, v_num=0, val_loss=0.16, val_cer=0.586]\n",
            "Validating:  69% 11/16 [00:08<00:03,  1.57it/s]\u001b[A\n",
            "Epoch 32:  97% 92/95 [00:37<00:01,  2.43it/s, loss=0.121, v_num=0, val_loss=0.16, val_cer=0.586]\n",
            "Validating:  81% 13/16 [00:09<00:01,  1.58it/s]\u001b[A\n",
            "Epoch 32:  99% 94/95 [00:39<00:00,  2.40it/s, loss=0.121, v_num=0, val_loss=0.16, val_cer=0.586]\n",
            "Validating:  94% 15/16 [00:10<00:00,  1.59it/s]\u001b[A\n",
            "Epoch 32: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.121, v_num=0, val_loss=0.164, val_cer=0.586]\n",
            "Epoch 32: 100% 95/95 [00:40<00:00,  2.33it/s, loss=0.121, v_num=0, val_loss=0.164, val_cer=0.586]\n",
            "EMNISTLinesDataset loading data from HDF5...\n",
            "Testing: 100% 16/16 [00:10<00:00,  1.56it/s]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': tensor(0.5858, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r569HJ-_y4e"
      },
      "source": [
        "# DATALOADER:0 TEST RESULTS\n",
        "# {'test_acc': tensor(0.9022, device='cuda:0'),\n",
        "#  'test_cer': tensor(0.1749, device='cuda:0')}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nceh4-bUA4D5",
        "outputId": "75595f26-1919-4d49-eca6-6315a3516e27"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.1)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.18)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz-zxssFA5D0",
        "outputId": "fe712b70-12d6-419f-85bc-2b648341ea14"
      },
      "source": [
        "%cd ../lab5/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/fsdl-text-recognizer-2021-labs/lab5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-0ACDeAFf8Z",
        "outputId": "376df2c5-c742-4570-fe7d-5b7280b2c9ed"
      },
      "source": [
        "!wandb init\n",
        "# key: 75866ccb1436e945c0b7132d19598808ed27b68f"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Enter a name for your first project: 75866ccb1436e945c0b7132d19598808ed27b68f\n",
            "\u001b[32mThis directory is configured!  Next, track a run:\n",
            "\u001b[0m* In your training script:\n",
            "    \u001b[1mimport wandb\u001b[0m\n",
            "    \u001b[1mwandb.init(project=\"75866ccb1436e945c0b7132d19598808ed27b68f\")\u001b[0m\n",
            "* then `\u001b[1mpython <train.py>\u001b[0m`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr9d3cbEFjpB",
        "outputId": "48ff0142-c77e-41b3-eace-5f565653237d"
      },
      "source": [
        "!python training/run_experiment.py --wandb --max_epochs=10 --gpus='0,' --num_workers=4 --data_class=EMNISTLines2 --model_class=LineCNNTransformer --loss=transformer"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkopfyf\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglad-breeze-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kopfyf/75866ccb1436e945c0b7132d19598808ed27b68f\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kopfyf/75866ccb1436e945c0b7132d19598808ed27b68f/runs/ufjsj9xl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210821_234524-ufjsj9xl\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You have set progress_bar_refresh_rate < 20 on Google Colab. This may crash. Consider using progress_bar_refresh_rate >= 20 in Trainer.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "EMNISTLines2 generating data for train...\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "tcmalloc: large alloc 2293760000 bytes == 0x5567a1248000 @  0x7f165801cb6b 0x7f165803c379 0x7f15aa11274e 0x7f15aa1147b6 0x7f1635682ed2 0x7f163596db03 0x7f1635945137 0x7f163596020c 0x7f163593c6ba 0x7f1635945137 0x7f163596020c 0x7f1635a2c00d 0x7f1635679955 0x7f1635b9fbc7 0x7f1635bee455 0x7f16352500ce 0x7f1635969623 0x7f1635942ed2 0x7f16352500ce 0x7f1635969623 0x7f1635a4fb36 0x7f164570f807 0x5567843b8010 0x5567843b7da0 0x55678442c2f9 0x5567843b965a 0x556784427b0e 0x5567843b965a 0x556784427d67 0x556784426c35 0x5567842f8eb1\n",
            "EMNISTLines2 generating data for val...\n",
            "EMNISTLines2 generating data for test...\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "\n",
            "    | Name                                                       | Type                    | Params\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "0   | model                                                      | LineCNNTransformer      | 4.3 M \n",
            "1   | model.line_cnn                                             | LineCNN                 | 1.6 M \n",
            "2   | model.line_cnn.convs                                       | Sequential              | 1.2 M \n",
            "3   | model.line_cnn.convs.0                                     | ConvBlock               | 320   \n",
            "4   | model.line_cnn.convs.0.conv                                | Conv2d                  | 320   \n",
            "5   | model.line_cnn.convs.0.relu                                | ReLU                    | 0     \n",
            "6   | model.line_cnn.convs.1                                     | ConvBlock               | 9.2 K \n",
            "7   | model.line_cnn.convs.1.conv                                | Conv2d                  | 9.2 K \n",
            "8   | model.line_cnn.convs.1.relu                                | ReLU                    | 0     \n",
            "9   | model.line_cnn.convs.2                                     | ConvBlock               | 9.2 K \n",
            "10  | model.line_cnn.convs.2.conv                                | Conv2d                  | 9.2 K \n",
            "11  | model.line_cnn.convs.2.relu                                | ReLU                    | 0     \n",
            "12  | model.line_cnn.convs.3                                     | ConvBlock               | 9.2 K \n",
            "13  | model.line_cnn.convs.3.conv                                | Conv2d                  | 9.2 K \n",
            "14  | model.line_cnn.convs.3.relu                                | ReLU                    | 0     \n",
            "15  | model.line_cnn.convs.4                                     | ConvBlock               | 18.5 K\n",
            "16  | model.line_cnn.convs.4.conv                                | Conv2d                  | 18.5 K\n",
            "17  | model.line_cnn.convs.4.relu                                | ReLU                    | 0     \n",
            "18  | model.line_cnn.convs.5                                     | ConvBlock               | 36.9 K\n",
            "19  | model.line_cnn.convs.5.conv                                | Conv2d                  | 36.9 K\n",
            "20  | model.line_cnn.convs.5.relu                                | ReLU                    | 0     \n",
            "21  | model.line_cnn.convs.6                                     | ConvBlock               | 73.9 K\n",
            "22  | model.line_cnn.convs.6.conv                                | Conv2d                  | 73.9 K\n",
            "23  | model.line_cnn.convs.6.relu                                | ReLU                    | 0     \n",
            "24  | model.line_cnn.convs.7                                     | ConvBlock               | 147 K \n",
            "25  | model.line_cnn.convs.7.conv                                | Conv2d                  | 147 K \n",
            "26  | model.line_cnn.convs.7.relu                                | ReLU                    | 0     \n",
            "27  | model.line_cnn.convs.8                                     | ConvBlock               | 918 K \n",
            "28  | model.line_cnn.convs.8.conv                                | Conv2d                  | 918 K \n",
            "29  | model.line_cnn.convs.8.relu                                | ReLU                    | 0     \n",
            "30  | model.line_cnn.fc1                                         | Linear                  | 262 K \n",
            "31  | model.line_cnn.dropout                                     | Dropout                 | 0     \n",
            "32  | model.line_cnn.fc2                                         | Linear                  | 131 K \n",
            "33  | model.embedding                                            | Embedding               | 21.2 K\n",
            "34  | model.fc                                                   | Linear                  | 21.3 K\n",
            "35  | model.pos_encoder                                          | PositionalEncoding      | 0     \n",
            "36  | model.pos_encoder.dropout                                  | Dropout                 | 0     \n",
            "37  | model.transformer_decoder                                  | TransformerDecoder      | 2.6 M \n",
            "38  | model.transformer_decoder.layers                           | ModuleList              | 2.6 M \n",
            "39  | model.transformer_decoder.layers.0                         | TransformerDecoderLayer | 659 K \n",
            "40  | model.transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 263 K \n",
            "41  | model.transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "42  | model.transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 263 K \n",
            "43  | model.transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "44  | model.transformer_decoder.layers.0.linear1                 | Linear                  | 65.8 K\n",
            "45  | model.transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
            "46  | model.transformer_decoder.layers.0.linear2                 | Linear                  | 65.8 K\n",
            "47  | model.transformer_decoder.layers.0.norm1                   | LayerNorm               | 512   \n",
            "48  | model.transformer_decoder.layers.0.norm2                   | LayerNorm               | 512   \n",
            "49  | model.transformer_decoder.layers.0.norm3                   | LayerNorm               | 512   \n",
            "50  | model.transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
            "51  | model.transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
            "52  | model.transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
            "53  | model.transformer_decoder.layers.1                         | TransformerDecoderLayer | 659 K \n",
            "54  | model.transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 263 K \n",
            "55  | model.transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "56  | model.transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 263 K \n",
            "57  | model.transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "58  | model.transformer_decoder.layers.1.linear1                 | Linear                  | 65.8 K\n",
            "59  | model.transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
            "60  | model.transformer_decoder.layers.1.linear2                 | Linear                  | 65.8 K\n",
            "61  | model.transformer_decoder.layers.1.norm1                   | LayerNorm               | 512   \n",
            "62  | model.transformer_decoder.layers.1.norm2                   | LayerNorm               | 512   \n",
            "63  | model.transformer_decoder.layers.1.norm3                   | LayerNorm               | 512   \n",
            "64  | model.transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
            "65  | model.transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
            "66  | model.transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
            "67  | model.transformer_decoder.layers.2                         | TransformerDecoderLayer | 659 K \n",
            "68  | model.transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 263 K \n",
            "69  | model.transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "70  | model.transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 263 K \n",
            "71  | model.transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "72  | model.transformer_decoder.layers.2.linear1                 | Linear                  | 65.8 K\n",
            "73  | model.transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
            "74  | model.transformer_decoder.layers.2.linear2                 | Linear                  | 65.8 K\n",
            "75  | model.transformer_decoder.layers.2.norm1                   | LayerNorm               | 512   \n",
            "76  | model.transformer_decoder.layers.2.norm2                   | LayerNorm               | 512   \n",
            "77  | model.transformer_decoder.layers.2.norm3                   | LayerNorm               | 512   \n",
            "78  | model.transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
            "79  | model.transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
            "80  | model.transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
            "81  | model.transformer_decoder.layers.3                         | TransformerDecoderLayer | 659 K \n",
            "82  | model.transformer_decoder.layers.3.self_attn               | MultiheadAttention      | 263 K \n",
            "83  | model.transformer_decoder.layers.3.self_attn.out_proj      | _LinearWithBias         | 65.8 K\n",
            "84  | model.transformer_decoder.layers.3.multihead_attn          | MultiheadAttention      | 263 K \n",
            "85  | model.transformer_decoder.layers.3.multihead_attn.out_proj | _LinearWithBias         | 65.8 K\n",
            "86  | model.transformer_decoder.layers.3.linear1                 | Linear                  | 65.8 K\n",
            "87  | model.transformer_decoder.layers.3.dropout                 | Dropout                 | 0     \n",
            "88  | model.transformer_decoder.layers.3.linear2                 | Linear                  | 65.8 K\n",
            "89  | model.transformer_decoder.layers.3.norm1                   | LayerNorm               | 512   \n",
            "90  | model.transformer_decoder.layers.3.norm2                   | LayerNorm               | 512   \n",
            "91  | model.transformer_decoder.layers.3.norm3                   | LayerNorm               | 512   \n",
            "92  | model.transformer_decoder.layers.3.dropout1                | Dropout                 | 0     \n",
            "93  | model.transformer_decoder.layers.3.dropout2                | Dropout                 | 0     \n",
            "94  | model.transformer_decoder.layers.3.dropout3                | Dropout                 | 0     \n",
            "95  | train_acc                                                  | Accuracy                | 0     \n",
            "96  | val_acc                                                    | Accuracy                | 0     \n",
            "97  | test_acc                                                   | Accuracy                | 0     \n",
            "98  | loss_fn                                                    | CrossEntropyLoss        | 0     \n",
            "99  | val_cer                                                    | CharacterErrorRate      | 0     \n",
            "100 | test_cer                                                   | CharacterErrorRate      | 0     \n",
            "---------------------------------------------------------------------------------------------------------\n",
            "4.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "4.3 M     Total params\n",
            "Epoch 0:  83% 79/95 [01:49<00:22,  1.39s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 0:  85% 81/95 [01:53<00:19,  1.41s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Validating:  12% 2/16 [00:06<00:43,  3.11s/it]\u001b[A\n",
            "Epoch 0:  87% 83/95 [01:59<00:17,  1.43s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Validating:  25% 4/16 [00:11<00:33,  2.76s/it]\u001b[A\n",
            "Epoch 0:  89% 85/95 [02:04<00:14,  1.46s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Validating:  38% 6/16 [00:16<00:26,  2.66s/it]\u001b[A\n",
            "Epoch 0:  92% 87/95 [02:09<00:11,  1.49s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Validating:  50% 8/16 [00:22<00:21,  2.63s/it]\u001b[A\n",
            "Epoch 0:  94% 89/95 [02:14<00:09,  1.51s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Validating:  62% 10/16 [00:27<00:15,  2.61s/it]\u001b[A\n",
            "Epoch 0:  96% 91/95 [02:19<00:06,  1.54s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Validating:  75% 12/16 [00:32<00:10,  2.61s/it]\u001b[A\n",
            "Epoch 0:  98% 93/95 [02:24<00:03,  1.56s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Validating:  88% 14/16 [00:37<00:05,  2.61s/it]\u001b[A\n",
            "Epoch 0: 100% 95/95 [02:30<00:00,  1.58s/it, loss=2.86, v_num=j9xl, val_loss=4.82, val_cer=0.999]\n",
            "Epoch 0: 100% 95/95 [02:32<00:00,  1.60s/it, loss=2.86, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Epoch 1:  84% 80/95 [01:57<00:22,  1.47s/it, loss=2.46, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:03<00:59,  3.96s/it]\u001b[A\n",
            "Epoch 1:  86% 82/95 [02:04<00:19,  1.52s/it, loss=2.46, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.95s/it]\u001b[A\n",
            "Epoch 1:  88% 84/95 [02:09<00:16,  1.54s/it, loss=2.46, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.78s/it]\u001b[A\n",
            "Epoch 1:  91% 86/95 [02:14<00:14,  1.57s/it, loss=2.46, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Validating:  44% 7/16 [00:19<00:24,  2.72s/it]\u001b[A\n",
            "Epoch 1:  93% 88/95 [02:20<00:11,  1.59s/it, loss=2.46, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Validating:  56% 9/16 [00:25<00:18,  2.70s/it]\u001b[A\n",
            "Epoch 1:  95% 90/95 [02:25<00:08,  1.62s/it, loss=2.46, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.69s/it]\u001b[A\n",
            "Epoch 1:  97% 92/95 [02:31<00:04,  1.64s/it, loss=2.46, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.69s/it]\u001b[A\n",
            "Epoch 1:  99% 94/95 [02:36<00:01,  1.66s/it, loss=2.46, v_num=j9xl, val_loss=2.65, val_cer=0.915]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.69s/it]\u001b[A\n",
            "Epoch 1: 100% 95/95 [02:41<00:00,  1.70s/it, loss=2.46, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "                                               \u001b[A/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Trying to log at a previous step. Use `commit=False` when logging metrics manually.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 157 < 165; dropping {'val_loss': 2.3638687133789062, 'val_cer': 0.8253723382949829, 'epoch': 1}.\n",
            "Epoch 2:  84% 80/95 [01:58<00:22,  1.48s/it, loss=2.35, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:03<00:58,  3.93s/it]\u001b[A\n",
            "Epoch 2:  86% 82/95 [02:05<00:19,  1.53s/it, loss=2.35, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.96s/it]\u001b[A\n",
            "Epoch 2:  88% 84/95 [02:10<00:17,  1.55s/it, loss=2.35, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.79s/it]\u001b[A\n",
            "Epoch 2:  91% 86/95 [02:15<00:14,  1.58s/it, loss=2.35, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "Validating:  44% 7/16 [00:20<00:24,  2.73s/it]\u001b[A\n",
            "Epoch 2:  93% 88/95 [02:21<00:11,  1.61s/it, loss=2.35, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "Validating:  56% 9/16 [00:25<00:18,  2.71s/it]\u001b[A\n",
            "Epoch 2:  95% 90/95 [02:26<00:08,  1.63s/it, loss=2.35, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.69s/it]\u001b[A\n",
            "Epoch 2:  97% 92/95 [02:32<00:04,  1.65s/it, loss=2.35, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.69s/it]\u001b[A\n",
            "Epoch 2:  99% 94/95 [02:37<00:01,  1.67s/it, loss=2.35, v_num=j9xl, val_loss=2.36, val_cer=0.825]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.68s/it]\u001b[A\n",
            "Epoch 2: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.35, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Epoch 3:  84% 80/95 [01:58<00:22,  1.48s/it, loss=2.3, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:04<01:00,  4.01s/it]\u001b[A\n",
            "Epoch 3:  86% 82/95 [02:05<00:19,  1.53s/it, loss=2.3, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.98s/it]\u001b[A\n",
            "Epoch 3:  88% 84/95 [02:10<00:17,  1.56s/it, loss=2.3, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.80s/it]\u001b[A\n",
            "Epoch 3:  91% 86/95 [02:16<00:14,  1.58s/it, loss=2.3, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Validating:  44% 7/16 [00:20<00:24,  2.73s/it]\u001b[A\n",
            "Epoch 3:  93% 88/95 [02:21<00:11,  1.61s/it, loss=2.3, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Validating:  56% 9/16 [00:25<00:18,  2.70s/it]\u001b[A\n",
            "Epoch 3:  95% 90/95 [02:26<00:08,  1.63s/it, loss=2.3, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.69s/it]\u001b[A\n",
            "Epoch 3:  97% 92/95 [02:32<00:04,  1.65s/it, loss=2.3, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.68s/it]\u001b[A\n",
            "Epoch 3:  99% 94/95 [02:37<00:01,  1.68s/it, loss=2.3, v_num=j9xl, val_loss=2.26, val_cer=0.814]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.68s/it]\u001b[A\n",
            "Epoch 3: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.3, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Epoch 4:  84% 80/95 [01:58<00:22,  1.48s/it, loss=2.24, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:03<00:58,  3.91s/it]\u001b[A\n",
            "Epoch 4:  86% 82/95 [02:05<00:19,  1.53s/it, loss=2.24, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.95s/it]\u001b[A\n",
            "Epoch 4:  88% 84/95 [02:10<00:17,  1.55s/it, loss=2.24, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.79s/it]\u001b[A\n",
            "Epoch 4:  91% 86/95 [02:15<00:14,  1.58s/it, loss=2.24, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Validating:  44% 7/16 [00:19<00:24,  2.73s/it]\u001b[A\n",
            "Epoch 4:  93% 88/95 [02:21<00:11,  1.60s/it, loss=2.24, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Validating:  56% 9/16 [00:25<00:18,  2.70s/it]\u001b[A\n",
            "Epoch 4:  95% 90/95 [02:26<00:08,  1.63s/it, loss=2.24, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.69s/it]\u001b[A\n",
            "Epoch 4:  97% 92/95 [02:31<00:04,  1.65s/it, loss=2.24, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.69s/it]\u001b[A\n",
            "Epoch 4:  99% 94/95 [02:37<00:01,  1.67s/it, loss=2.24, v_num=j9xl, val_loss=2.16, val_cer=0.825]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.68s/it]\u001b[A\n",
            "Epoch 4: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.24, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Epoch 5:  84% 80/95 [01:58<00:22,  1.48s/it, loss=2.2, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:03<00:59,  3.97s/it]\u001b[A\n",
            "Epoch 5:  86% 82/95 [02:05<00:19,  1.53s/it, loss=2.2, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.97s/it]\u001b[A\n",
            "Epoch 5:  88% 84/95 [02:10<00:17,  1.55s/it, loss=2.2, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.79s/it]\u001b[A\n",
            "Epoch 5:  91% 86/95 [02:15<00:14,  1.58s/it, loss=2.2, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Validating:  44% 7/16 [00:20<00:24,  2.73s/it]\u001b[A\n",
            "Epoch 5:  93% 88/95 [02:21<00:11,  1.61s/it, loss=2.2, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Validating:  56% 9/16 [00:25<00:18,  2.70s/it]\u001b[A\n",
            "Epoch 5:  95% 90/95 [02:26<00:08,  1.63s/it, loss=2.2, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.69s/it]\u001b[A\n",
            "Epoch 5:  97% 92/95 [02:32<00:04,  1.65s/it, loss=2.2, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.69s/it]\u001b[A\n",
            "Epoch 5:  99% 94/95 [02:37<00:01,  1.67s/it, loss=2.2, v_num=j9xl, val_loss=2.12, val_cer=0.806]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.68s/it]\u001b[A\n",
            "Epoch 5: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.2, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Epoch 6:  84% 80/95 [01:58<00:22,  1.48s/it, loss=2.17, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:03<00:59,  3.98s/it]\u001b[A\n",
            "Epoch 6:  86% 82/95 [02:05<00:19,  1.53s/it, loss=2.17, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.97s/it]\u001b[A\n",
            "Epoch 6:  88% 84/95 [02:10<00:17,  1.56s/it, loss=2.17, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.80s/it]\u001b[A\n",
            "Epoch 6:  91% 86/95 [02:16<00:14,  1.58s/it, loss=2.17, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Validating:  44% 7/16 [00:20<00:24,  2.74s/it]\u001b[A\n",
            "Epoch 6:  93% 88/95 [02:21<00:11,  1.61s/it, loss=2.17, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Validating:  56% 9/16 [00:25<00:18,  2.71s/it]\u001b[A\n",
            "Epoch 6:  95% 90/95 [02:26<00:08,  1.63s/it, loss=2.17, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.70s/it]\u001b[A\n",
            "Epoch 6:  97% 92/95 [02:32<00:04,  1.65s/it, loss=2.17, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.69s/it]\u001b[A\n",
            "Epoch 6:  99% 94/95 [02:37<00:01,  1.68s/it, loss=2.17, v_num=j9xl, val_loss=2.11, val_cer=0.799]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.69s/it]\u001b[A\n",
            "Epoch 6: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.17, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "                                               \u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 552 < 565; dropping {'val_loss': 2.0707690715789795, 'val_cer': 0.8045512437820435, 'epoch': 6}.\n",
            "Epoch 7:  84% 80/95 [01:58<00:22,  1.48s/it, loss=2.14, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:04<01:00,  4.02s/it]\u001b[A\n",
            "Epoch 7:  86% 82/95 [02:05<00:19,  1.53s/it, loss=2.14, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.99s/it]\u001b[A\n",
            "Epoch 7:  88% 84/95 [02:10<00:17,  1.55s/it, loss=2.14, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.81s/it]\u001b[A\n",
            "Epoch 7:  91% 86/95 [02:15<00:14,  1.58s/it, loss=2.14, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "Validating:  44% 7/16 [00:20<00:24,  2.75s/it]\u001b[A\n",
            "Epoch 7:  93% 88/95 [02:21<00:11,  1.61s/it, loss=2.14, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "Validating:  56% 9/16 [00:25<00:19,  2.72s/it]\u001b[A\n",
            "Epoch 7:  95% 90/95 [02:26<00:08,  1.63s/it, loss=2.14, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.71s/it]\u001b[A\n",
            "Epoch 7:  97% 92/95 [02:32<00:04,  1.65s/it, loss=2.14, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.70s/it]\u001b[A\n",
            "Epoch 7:  99% 94/95 [02:37<00:01,  1.68s/it, loss=2.14, v_num=j9xl, val_loss=2.07, val_cer=0.805]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.70s/it]\u001b[A\n",
            "Epoch 7: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.14, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Epoch 8:  84% 80/95 [01:58<00:22,  1.48s/it, loss=2.09, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:04<01:00,  4.03s/it]\u001b[A\n",
            "Epoch 8:  86% 82/95 [02:05<00:19,  1.53s/it, loss=2.09, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.99s/it]\u001b[A\n",
            "Epoch 8:  88% 84/95 [02:10<00:17,  1.56s/it, loss=2.09, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.81s/it]\u001b[A\n",
            "Epoch 8:  91% 86/95 [02:16<00:14,  1.58s/it, loss=2.09, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Validating:  44% 7/16 [00:20<00:24,  2.75s/it]\u001b[A\n",
            "Epoch 8:  93% 88/95 [02:21<00:11,  1.61s/it, loss=2.09, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Validating:  56% 9/16 [00:25<00:19,  2.72s/it]\u001b[A\n",
            "Epoch 8:  95% 90/95 [02:26<00:08,  1.63s/it, loss=2.09, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.70s/it]\u001b[A\n",
            "Epoch 8:  97% 92/95 [02:32<00:04,  1.66s/it, loss=2.09, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.70s/it]\u001b[A\n",
            "Epoch 8:  99% 94/95 [02:37<00:01,  1.68s/it, loss=2.09, v_num=j9xl, val_loss=2.03, val_cer=0.793]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.70s/it]\u001b[A\n",
            "Epoch 8: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.09, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "                                               \u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step must only increase in log calls.  Step 710 < 715; dropping {'val_loss': 1.9910086393356323, 'val_cer': 0.7962304949760437, 'epoch': 8}.\n",
            "Epoch 9:  84% 80/95 [01:58<00:22,  1.48s/it, loss=2.06, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   6% 1/16 [00:04<01:00,  4.00s/it]\u001b[A\n",
            "Epoch 9:  86% 82/95 [02:05<00:19,  1.53s/it, loss=2.06, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "Validating:  19% 3/16 [00:09<00:38,  2.99s/it]\u001b[A\n",
            "Epoch 9:  88% 84/95 [02:10<00:17,  1.55s/it, loss=2.06, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "Validating:  31% 5/16 [00:14<00:30,  2.81s/it]\u001b[A\n",
            "Epoch 9:  91% 86/95 [02:15<00:14,  1.58s/it, loss=2.06, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "Validating:  44% 7/16 [00:20<00:24,  2.75s/it]\u001b[A\n",
            "Epoch 9:  93% 88/95 [02:21<00:11,  1.61s/it, loss=2.06, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "Validating:  56% 9/16 [00:25<00:19,  2.72s/it]\u001b[A\n",
            "Epoch 9:  95% 90/95 [02:26<00:08,  1.63s/it, loss=2.06, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "Validating:  69% 11/16 [00:30<00:13,  2.71s/it]\u001b[A\n",
            "Epoch 9:  97% 92/95 [02:32<00:04,  1.65s/it, loss=2.06, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "Validating:  81% 13/16 [00:36<00:08,  2.71s/it]\u001b[A\n",
            "Epoch 9:  99% 94/95 [02:37<00:01,  1.68s/it, loss=2.06, v_num=j9xl, val_loss=1.99, val_cer=0.796]\n",
            "Validating:  94% 15/16 [00:41<00:02,  2.71s/it]\u001b[A\n",
            "Epoch 9: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.06, v_num=j9xl, val_loss=1.95, val_cer=0.781]\n",
            "Epoch 9: 100% 95/95 [02:42<00:00,  1.71s/it, loss=2.06, v_num=j9xl, val_loss=1.95, val_cer=0.781]\n",
            "EMNISTLines2 loading data from HDF5...\n",
            "Testing: 100% 16/16 [00:40<00:00,  2.50s/it]\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_cer': tensor(0.7763, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Best model saved at: training/logs/75866ccb1436e945c0b7132d19598808ed27b68f/ufjsj9xl/checkpoints/epoch=009-val_loss=1.953-val_cer=0.781.ckpt\n",
            "Best model also uploaded to W&B\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4803\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210821_234524-ufjsj9xl/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/fsdl-text-recognizer-2021-labs/lab5/wandb/run-20210821_234524-ufjsj9xl/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                _runtime 1720\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _timestamp 1629591244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   _step 1579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              train_loss 2.04645\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                   epoch 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                val_loss 1.95282\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                 val_cer 0.78126\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                test_cer 0.77626\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     _runtime ▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   _timestamp ▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        _step ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss █▄▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        epoch ▁▁▂▂▃▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_loss █▄▃▃▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      val_cer █▃▃▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     test_cer ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 178 media file(s), 0 artifact file(s) and 1 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mglad-breeze-1\u001b[0m: \u001b[34mhttps://wandb.ai/kopfyf/75866ccb1436e945c0b7132d19598808ed27b68f/runs/ufjsj9xl\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5-VkOOqGqtZ"
      },
      "source": [
        "# wandb: Tracking run with wandb version 0.10.14\n",
        "# wandb: Syncing run rosy-shape-1\n",
        "# wandb: ⭐️ View project at https://wandb.ai/USER/PROJECT\n",
        "# wandb: 🚀 View run at https://wandb.ai/USER/PROJECT/runs/35hje5h9\n",
        "\n",
        "# https://wandb.ai/kopfyf/75866ccb1436e945c0b7132d19598808ed27b68f/runs/ufjsj9xl?workspace=user-kopfyf\n",
        "\n",
        "'''\n",
        "Privacy\n",
        "PRIVATE\n",
        "Tags\n",
        "Author\n",
        "kopfyf\n",
        "State\n",
        "running\n",
        "Start time\n",
        "August 21st, 2021 at 7:45:25 pm\n",
        "Duration\n",
        "8m 1s\n",
        "Run path\n",
        "kopfyf/75866ccb1436e945c0b7132d19598808ed27b68f/ufjsj9xl\n",
        "Hostname\n",
        "75e7061f0f27\n",
        "OS\n",
        "Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\n",
        "Python version\n",
        "3.7.11\n",
        "Python executable\n",
        "/usr/bin/python3\n",
        "Git repository\n",
        "git clone https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2021-labs\n",
        "Git state\n",
        "git checkout -b \"glad-breeze-1\" 174ebbdc065442175d9457b7a97d6e065f3d9cd0\n",
        "Command\n",
        "training/run_experiment.py --wandb --max_epochs=10 --gpus=0, --num_workers=4 --data_class=EMNISTLines2 --model_class=LineCNNTransformer --loss=transformer\n",
        "System Hardware\n",
        "CPU count\t2\n",
        "GPU count\t1\n",
        "GPU type\tTesla T4\n",
        "W&B CLI Version\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifoAbExfJwLx",
        "outputId": "c2b29213-f7d7-466a-b0d8-af197eaa4c17"
      },
      "source": [
        "# setup an initial configuration file for sweeps in training/emnist_lines_line_cnn_transformer_sweep.yaml.\n",
        "# we can run experiments that are stored on Weights & Biases, we can more quickly find a good operating point, where your model is converging to something.\n",
        "# Then, we'd like to search a bit around that point -- this is often called hyper-parameter optimization.\n",
        "!wandb sweep training/sweeps/emnist_lines2_line_cnn_transformer.yml"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: training/sweeps/emnist_lines2_line_cnn_transformer.yml\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Created sweep with ID: \u001b[33m96bela4n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/kopfyf/75866ccb1436e945c0b7132d19598808ed27b68f/sweeps/96bela4n\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent kopfyf/75866ccb1436e945c0b7132d19598808ed27b68f/96bela4n\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd0Re2CJMKfb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}